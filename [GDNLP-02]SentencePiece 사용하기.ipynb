{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assumed-garbage",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#2-5.-프로젝트:-SentencePiece-사용하기\" data-toc-modified-id=\"2-5.-프로젝트:-SentencePiece-사용하기-1\">2-5. 프로젝트: SentencePiece 사용하기</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Step-1.-SentencePiece-설치하기\" data-toc-modified-id=\"Step-1.-SentencePiece-설치하기-1.0.1\">Step 1. SentencePiece 설치하기</a></span></li><li><span><a href=\"#Step-2.-SentencePiece-모델-학습\" data-toc-modified-id=\"Step-2.-SentencePiece-모델-학습-1.0.2\">Step 2. SentencePiece 모델 학습</a></span></li><li><span><a href=\"#Step-3.-Tokenizer-함수-작성\" data-toc-modified-id=\"Step-3.-Tokenizer-함수-작성-1.0.3\">Step 3. Tokenizer 함수 작성</a></span></li><li><span><a href=\"#Step-4.-네이버-영화리뷰-감정분석-문제에-SentencePiece-적용해-보기\" data-toc-modified-id=\"Step-4.-네이버-영화리뷰-감정분석-문제에-SentencePiece-적용해-보기-1.0.4\">Step 4. 네이버 영화리뷰 감정분석 문제에 SentencePiece 적용해 보기</a></span><ul class=\"toc-item\"><li><span><a href=\"#네이버-영화리뷰-감정분석-코퍼스에-sentencepiece를-적용시킨-모델-학습하기\" data-toc-modified-id=\"네이버-영화리뷰-감정분석-코퍼스에-sentencepiece를-적용시킨-모델-학습하기-1.0.4.1\">네이버 영화리뷰 감정분석 코퍼스에 sentencepiece를 적용시킨 모델 학습하기</a></span></li><li><span><a href=\"#학습된-모델로-sp_tokenize()-메소드-구현하기\" data-toc-modified-id=\"학습된-모델로-sp_tokenize()-메소드-구현하기-1.0.4.2\">학습된 모델로 sp_tokenize() 메소드 구현하기</a></span></li><li><span><a href=\"#구현된-토크나이저를-적용하여-네이버-영화리뷰-감정분석-모델을-재학습하기\" data-toc-modified-id=\"구현된-토크나이저를-적용하여-네이버-영화리뷰-감정분석-모델을-재학습하기-1.0.4.3\">구현된 토크나이저를 적용하여 네이버 영화리뷰 감정분석 모델을 재학습하기</a></span></li><li><span><a href=\"#KoNLPy-형태소-분석기를-사용한-모델과-성능-비교하기\" data-toc-modified-id=\"KoNLPy-형태소-분석기를-사용한-모델과-성능-비교하기-1.0.4.4\">KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기</a></span></li><li><span><a href=\"#(보너스)-SentencePiece-모델의-model_type,-vocab_size-등을-변경해-가면서-성능-개선-여부-확인하기\" data-toc-modified-id=\"(보너스)-SentencePiece-모델의-model_type,-vocab_size-등을-변경해-가면서-성능-개선-여부-확인하기-1.0.4.5\">(보너스) SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기</a></span></li></ul></li><li><span><a href=\"#루브릭\" data-toc-modified-id=\"루브릭-1.0.5\">루브릭</a></span></li><li><span><a href=\"#Postscript\" data-toc-modified-id=\"Postscript-1.0.6\">Postscript</a></span></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-1.0.7\">Reference</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-comfort",
   "metadata": {},
   "source": [
    "# 2-5. 프로젝트: SentencePiece 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-investing",
   "metadata": {},
   "source": [
    "### Step 1. SentencePiece 설치하기\n",
    "---\n",
    "SentencePiece는 SentencePiece는 Google에서 제공하는 오픈소스 기반 Sentence Tokenizer/Detokenizer 로서, BPE와 unigram 2가지 subword 토크나이징 모델 중 하나를 선택해서 사용할 수 있도록 패키징한 것입니다. 아래 링크의 페이지에서 상세한 내용을 파악할 수 있습니다.\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "위 페이지의 서두에서도 언급하고 있듯, SentencePiece는 딥러닝 자연어처리 모델의 앞부분에 사용할 목적으로 최적화되어 있는데, 최근 pretrained model들이 거의 대부분 SentencePiece를 tokenizer로 채용하면서 사실상 표준의 역할을 하고 있습니다. 앞으로의 실습 과정에서 자주 만나게 될 것이므로 꼭 친숙해지시기를 당부드립니다.\n",
    "\n",
    "다음과 같이 설치를 진행합니다. SentencePiece는 python에서 쓰라고 만들어진 라이브러리는 아니지만 편리한 파이썬 wrapper를 아래와 같이 제공하고 있습니다.\n",
    "\n",
    "```\n",
    "$ pip install sentencepiece\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-outreach",
   "metadata": {},
   "source": [
    "### Step 2. SentencePiece 모델 학습\n",
    "---\n",
    "앞서 배운 `tokenize()` 함수를 기억하나요? 다시 한번 상기시켜드릴게요!\n",
    "\n",
    "```\n",
    "def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "```\n",
    "\n",
    "위와 같이 `tf.keras.preprocessing.text.Tokenizer`에 corpus를 주고 `tokenizer.fit_on_texts(corpus)`을 하면 토크나이저 내부적으로 단어사전과 토크나이저 기능을 corpus에 맞춤형으로 자동생성해 주는 것입니다.\n",
    "\n",
    "그럼 이를 위해서 SentencePiece 모델을 학습하는 과정을 거쳐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-arlington",
   "metadata": {},
   "source": [
    "```\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "temp_file = os.getenv(\n",
    "    'HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(\n",
    "        temp_file, vocab_size)\n",
    ")\n",
    "# 위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-bankruptcy",
   "metadata": {},
   "source": [
    "위 코드를 실행하면 정상적으로 SentencePiece 모델 학습이 완료된 후 korean_spm.model 파일과 korean_spm.vocab vocabulary 파일이 생성되었음을 확인할 수 있습니다.\n",
    "\n",
    "그럼 이렇게 학습된 SentencePiece 모델을 어떻게 활용하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "genuine-living",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1244, 11, 306, 7, 3599, 11, 286, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.', 1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-rendering",
   "metadata": {},
   "source": [
    "어떻습니까? SentencePiece의 토크나이징 실력이 괜찮은 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-prevention",
   "metadata": {},
   "source": [
    "### Step 3. Tokenizer 함수 작성\n",
    "---\n",
    "우리는 위에서 훈련시킨 SentencePiece를 활용하여 위 함수와 유사한 기능을 하는 `sp_tokenize()` 함수를 정의할 겁니다. 하지만 _SentencePiece_ 가 동작하는 방식이 단순 토큰화와는 달라 완전히 동일하게는 정의하기 어렵습니다. 그러니 아래 조건을 만족하는 함수를 정의하도록 하습니다.\n",
    "\n",
    ">1) _매개변수로 토큰화된 문장의 `list`를 전달하는 대신 **온전한 문장**의 `list` 를 전달합니다._  \n",
    ">\n",
    ">2) _**생성된 vocab 파일**을 읽어와 `{ <word> : <idx> }` 형태를 가지는 `word_index` 사전과 `{ <idx> : <word> }` 형태를 가지는 `index_word` 사전을 생성하고 함께 **반환**합니다._  \n",
    ">\n",
    ">3) _리턴값인 `tensor` 는 앞의 함수와 동일하게 토큰화한 후 Encoding된 문장입니다. 바로 학습에 사용할 수 있게 Padding은 당연히 해야겠죠?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ranking-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attended-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx: word})\n",
    "        index_word.update({word: idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "combined-sleep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1949 5662    5    4 7975 1983    3    0    0    0    0    0    0    0]\n",
      " [ 107 1638  101    4    0  419   11    4   14    0 1969    3    3    3]]\n"
     ]
    }
   ],
   "source": [
    "# sp_tokenize(s, corpus) 사용예제\n",
    "\n",
    "my_corpus = ['나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-motel",
   "metadata": {},
   "source": [
    "### Step 4. 네이버 영화리뷰 감정분석 문제에 SentencePiece 적용해 보기\n",
    "---\n",
    "아마 여러분들은 [네이버 영화리뷰 감정분석 태스크](https://github.com/e9t/nsmc/)를 한 번쯤은 다루어 보았을 것입니다. 한국어로 된 corpus를 다루어야 하므로 주로 KoNLPy에서 제공하는 형태소 분석기를 사용하여 텍스트를 전처리해서 RNN 모델을 분류기로 사용했을 것입니다.\n",
    "\n",
    "만약 이 문제에서 tokenizer를 sentencepiece로 바꾸어 다시 풀어본다면 더 성능이 좋아질까요? 비교해 보는 것도 흥미로울 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-piano",
   "metadata": {},
   "source": [
    "##### 1) 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thermal-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spatial-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 150000\n",
      "테스트용 리뷰 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/ratings_test.txt')\n",
    "print(f'훈련용 리뷰 개수 : {len(train_data)}')\n",
    "print(f'테스트용 리뷰 개수 : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-array",
   "metadata": {},
   "source": [
    "##### 2) 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lucky-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "train_data = train_data.dropna(how='any')  # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any())  # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demographic-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 샘플의 수 : 146182\n"
     ]
    }
   ],
   "source": [
    "print(f'훈련용 샘플의 수 : {len(train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "essential-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data에서 해당 리뷰의 긍/부정 유무가 기재되어있는 레이블(label) 값의 분포를 보겠습니다.\n",
    "train_data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "early-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 49157\n"
     ]
    }
   ],
   "source": [
    "test_data.drop_duplicates(subset=['document'], inplace=True)# document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print(f'전처리 후 테스트용 샘플의 개수 : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-accommodation",
   "metadata": {},
   "source": [
    "##### 3) 토큰화, 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-mixer",
   "metadata": {},
   "source": [
    "#### 네이버 영화리뷰 감정분석 코퍼스에 sentencepiece를 적용시킨 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hundred-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/'\n",
    "model_prefix = 'ratings_train_spm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "median-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_txt(file_path, file_name, target_name):\n",
    "    # 네이버 영화리뷰 감정분석 코퍼스 파일에서 텍스트 부분만 추출해 타겟파일에 저장합니다.\n",
    "    with open(file_path+file_name, 'r') as f:\n",
    "        data = [line.split('\\t')[1] for line in f.read().splitlines()[1:]]\n",
    "\n",
    "    with open(file_path+target_name, 'w') as f:\n",
    "        for row in data:\n",
    "            f.write(str(row)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "architectural-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings.txt의 텍스트를 추출해 텍스트 파일로 저장합니다.\n",
    "make_txt(file_path, 'ratings_train.txt', 'ratings_train_document.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "great-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000  # 16000, 32000\n",
    "\n",
    "# SentencePiece 모델을 학습합니다.\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f'--input={file_path}ratings_train_document.txt ' +\n",
    "    f'--model_prefix={model_prefix} ' +\n",
    "    f'--vocab_size={vocab_size} ' +\n",
    "    '--pad_id=0 --pad_piece=[PAD] ' +  # pad (0)\n",
    "    '--unk_id=1 --unk_piece=[UNK] ' +  # unknown (1)\n",
    "    '--bos_id=2 --bos_piece=[BOS] ' +  # begin of sequence (2)\n",
    "    '--eos_id=3 --eos_piece=[EOS]'  # end of sequence (3)\n",
    ")\n",
    "# 위 Train에서 --model_type='unigram'이 디폴트 적용되어 있습니다.\n",
    "# --model_type='bpe' 로 옵션을 주어 변경할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-sauce",
   "metadata": {},
   "source": [
    "#### 학습된 모델로 sp_tokenize() 메소드 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "absolute-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus, model_prefix):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open('./' + model_prefix + '.vocab', 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx: word})\n",
    "        index_word.update({word: idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor, padding='post')\n",
    "    \n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "statewide-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1403, 11, 394, 16, 1333, 11, 138, 18, 5]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s.Load('ratings_train_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.', 1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "false-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, word_index, index_word = sp_tokenize(\n",
    "    s, train_data['document'], model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vertical-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sen in test_data['document']:\n",
    "    X_test.append(s.EncodeAsIds(sen))\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=len(X_train[0]), padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "residential-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-arrow",
   "metadata": {},
   "source": [
    "#### 구현된 토크나이저를 적용하여 네이버 영화리뷰 감정분석 모델을 재학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "responsible-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "billion-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('GDNLP_02_sp_best_model.h5', monitor='val_acc',\n",
    "                     mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "regional-interpretation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "answering-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1828/1828 [==============================] - 36s 16ms/step - loss: 0.6933 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50388, saving model to GDNLP_02_sp_best_model.h5\n",
      "Epoch 2/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50388\n",
      "Epoch 3/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4961\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50388\n",
      "Epoch 4/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50388\n",
      "Epoch 5/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50388\n",
      "Epoch 6/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.4961\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50388\n",
      "Epoch 7/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.4961\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50388\n",
      "Epoch 8/15\n",
      "1828/1828 [==============================] - 28s 15ms/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.50388\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "practical-lewis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 10s 6ms/step - loss: 0.6932 - acc: 0.4973\n",
      "\n",
      " 테스트 정확도 : 0.4973\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('GDNLP_02_sp_best_model.h5')\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "normal-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    encoded = []\n",
    "    encoded.append(s.EncodeAsIds(new_sentence))  # 토큰화, 정수 인코딩\n",
    "    pad_new = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded, maxlen=len(X_train[0]), padding='post')  # 패딩\n",
    "    score = float(loaded_model.predict(pad_new))  # 예측\n",
    "    if(score > 0.5):\n",
    "        print('{:.2f}% 확률로 긍정 리뷰입니다.\\n'.format(score*100))\n",
    "    else:\n",
    "        print('{:.2f}% 확률로 부정 리뷰입니다.\\n'.format((1-score)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "impossible-wholesale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.46% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "innocent-december",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.46% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "prerequisite-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.46% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "external-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.46% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('감독 뭐하는 놈이냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "recreational-cemetery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.46% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-project",
   "metadata": {},
   "source": [
    "#### KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-pioneer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-thanks",
   "metadata": {},
   "source": [
    "#### (보너스) SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-tender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "smaller-thursday",
   "metadata": {},
   "source": [
    "- Word Vector는 활용할 필요가 없습니다. 활용이 가능하지도 않을 것입니다.\n",
    "- 머지않아 SentencePiece와 BERT 등의 pretrained 모델을 함께 활용하는 태스크를 다루게 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-shanghai",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "---\n",
    "\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "|평가문항|상세기준|\n",
    "|:---|:---|\n",
    "|1. SentencePiece를 이용하여 모델을 만들기까지의 과정이 정상적으로 진행되었는가?|코퍼스 분석, 전처리, SentencePiece 적용, 토크나이저 구현 및 동작이 빠짐없이 진행되었는가?|\n",
    "|2. SentencePiece를 통해 만든 Tokenizer가 자연어처리 모델과 결합하여 동작하는가?|SentencePiece 토크나이저가 적용된 Text Classifier 모델이 정상적으로 수렴하여 80% 이상의 test accuracy가 확인되었다.|\n",
    "|3. SentencePiece의 성능을 다각도로 비교분석하였는가?|SentencePiece 토크나이저를 활용했을 때의 성능을 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교하여 분석을 체계적으로 진행하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-macedonia",
   "metadata": {},
   "source": [
    "### Postscript\n",
    "---\n",
    "- row data에 대한 성능을 살펴보기 위해 특수문자 등을 제거하지 않았는데, 그래서 그런지 성능이 생각보다 안 좋게 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-craft",
   "metadata": {},
   "source": [
    "### Reference\n",
    "---\n",
    "[6) 네이버 영화 리뷰 감성 분류하기(Naver Movie Review Sentiment Analysis) - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/44249)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "146.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
